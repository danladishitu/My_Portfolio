{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Player Scouting and Talent Acquisition for Football Managers using AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the use of the FIFA19 dataset, the proposed AI model solves the difficulties managers have while attempting to choose the best players, as well as identifying the average, underperforming, undervalued, and overpriced players.\n",
    "\n",
    "\n",
    "**The structure of the model is described in two phases:**\n",
    "\n",
    "The first phase is to effectively build a model capable of grouping players based on their similarity in traits. To do this, I have implemented K-means, K-means++ and DBSCAN algorithms to group players based on their individual abilities, as well as noise removal from the dataset. The model can potentially identify patterns those certain players share in ways that would not normally have been considered by the team managers during their manual evaluation.\n",
    "\n",
    "The second phase entails building a classification model that will be capable of re-evaluating the players based on the cluster labels provided by the clustering algorithm in the first phase. These classifiers will be able to predict what group a fresh set of players will belong to. Support Vector Machine and Random Forest are two ML algorithms that I used for this. This would also help managers diagnose lack of skill diversity, identify under-priced and over-priced players, and potentially influence their transfer decisions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing all of the libraries required to build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, pandas as pd, seaborn as sns, matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "from sklearn import preprocessing\n",
    "import itertools\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "from imblearn.under_sampling import EditedNearestNeighbours \n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.utils import shuffle\n",
    "import plotly.graph_objs as go\n",
    "from itertools import product\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the libraries required to evaluate the classifiers' accuracy and performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import jaccard_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Dataset\n",
    "\n",
    "The dataset in use was obtained from Kaggle, which can be accessed online. Please, click on the link to download the dataset. https://www.kaggle.com/karangadiya/fifa19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset contains 18,207 rows and 89 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the percentage of empty rows also known as NaN (not a number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test = pd.concat([data.drop('Photo', axis = 1)], keys = ['data'], axis = 0)\n",
    "missing_values = pd.concat([train_test.isna().sum(),\n",
    "                            (train_test.isna().sum() / train_test.shape[0]) * 100], axis = 1, \n",
    "                           keys = ['Values missing', 'Percent of missing'])\n",
    "missing_values.loc[missing_values['Percent of missing'] > 0].sort_values(ascending = False, by = 'Percent of missing').style.background_gradient('Blues')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove columns that will not be used in the model, I need replace the column identifier ignoring spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [c.replace(' ', '') for c in data.columns]\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Position\n",
    "The first way to examining the ground truth is to check the player's position with the greatest number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x = data['Position'])\n",
    "plt.figure(figsize=(80, 40))\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "ax.set_xlabel('Position') \n",
    "ax.set_ylabel('Number of players')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age\n",
    "Every football player must be considered by their age. It contributes to their market value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x = data['Age'])\n",
    "plt.figure(figsize=(80, 40))\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "ax.set_xlabel('Age') \n",
    "ax.set_ylabel('Number of players')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potential \n",
    "Every football player has a unique quality called potential. It describes their expertise, which highly contributes to their market value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x = data['Potential'])\n",
    "plt.figure(figsize=(80, 40))\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "ax.set_xlabel('Potential') \n",
    "ax.set_ylabel('Number of Player')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any column that aren't necessary for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['Name','Unnamed:0','ID','Photo','Flag','Overall','ClubLogo', 'Special', 'InternationalReputation', 'WeakFoot',\n",
    "               'SkillMoves','WorkRate','BodyType','RealFace','JerseyNumber','Joined','LoanedFrom','ContractValidUntil',\n",
    "                'Weight','Crossing','Finishing','HeadingAccuracy','ShortPassing','Volleys','Dribbling','Curve','FKAccuracy',\n",
    "                'LongPassing','BallControl','Acceleration','SprintSpeed','Agility','Reactions','Balance','ShotPower','Jumping',\n",
    "                'Stamina','Strength','LongShots','Aggression','Interceptions','Positioning','Vision','Penalties','Composure',\n",
    "                'Marking','SlidingTackle','StandingTackle','ReleaseClause'], axis=1)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill up the empty rows (NaN) with the dataset's mean value\n",
    "There are many characteristics that are unimportant for goalkeepers, which is why some of their rows were empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_means = data.mean()\n",
    "data = data.fillna(column_means)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the Pounds symbol and letters from the players' wages and values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Wage = data.Wage.str.replace(\"€\",\"\")\n",
    "data.Wage = data.Wage.str.replace(\"K\",\"\").astype(\"float\")\n",
    "data.Wage.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Value = data.Value.str.replace(\"€\",\"\")\n",
    "data.Value = data.Value.str.replace(\"M\",\"\")\n",
    "data.Value = data.Value.str.replace(\"K\",\"\").astype(\"float\")\n",
    "data.Value.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "This is used to convert all categorical variable into indicator variable i.e., (0's and 1's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data)\n",
    "dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the dummy method into variable X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dummies\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the dataset\n",
    "Now let's normalize the dataset. But why do i need normalization in the first place? Normalization is a statistical method that helps mathematical-based algorithms to interpret features with different magnitudes and distributions equally. I used StandardScaler() to normalize the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_scale = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "players_scale[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Store the scaled data into a dataframe object\n",
    "df_players = pd.DataFrame(players_scale, columns=X.columns)\n",
    "df_players.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Means Algorithm\n",
    "\n",
    "## The following is the analytical strategy used in the K-means experiment:\n",
    "\n",
    "### 1. Applying the elbow method to determine the optimal number of K using the silhouette coefficient\n",
    "### 2. Applying K-means++ to the original dataset \n",
    "### 3. Hyperparameter tuning for K-means\n",
    "### 4. Applying PCA to K-means++\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Applying the elbow method to determine the optimal number of K using the silhouette coefficient\n",
    "inertia: (sum of squared error between each point and its cluster center) as a function of the number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = []\n",
    "k_list = range(1, 15)\n",
    "\n",
    "for k in k_list:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(df_players)\n",
    "    inertia.append([k, km.inertia_])\n",
    "    \n",
    "pca_results = pd.DataFrame({'Cluster': range(1,15), 'SSE': inertia})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(pd.DataFrame(inertia)[0], pd.DataFrame(inertia)[1], marker='o', color='green')\n",
    "plt.title('Optimal Number of Clusters using Elbow Method (Original Data)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Applying K-means++ to the original dataset\n",
    "\n",
    "The plot indicates that the number of clusters should be between 4 and 5, but for the purpose of simplicity, I chose 4 as my preferred number. Compute the sihouette score using k-means++ on the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_scale = KMeans(n_clusters=4, n_init=100, max_iter=400, init='k-means++').fit(df_players)\n",
    "print('KMeans Scaled Silhouette Score: {}'.format(silhouette_score(df_players, \n",
    "                                                                   kmeans_scale.labels_, metric='euclidean')))\n",
    "labels_scale = kmeans_scale.labels_\n",
    "clusters_scale = pd.concat([df_players, pd.DataFrame({'cluster_scaled':labels_scale})], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hyperparameter tuning for K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_clusters': [2, 3, 4, 5, 10, 20, 30]}\n",
    "\n",
    "parameter_grid = ParameterGrid(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -1\n",
    "model = KMeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Fine-tune the K-means model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in parameter_grid:\n",
    "    model.set_params(**g)\n",
    "    model.fit(df_players)\n",
    "\n",
    "    ss = metrics.silhouette_score(df_players, model.labels_)\n",
    "    print('Parameter: ', g, 'Score: ', ss)\n",
    "    if ss > best_score:\n",
    "        best_score = ss\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. Get the best silhouette score along with the number of clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. A scatter plot of the original dataset using K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_scale=k_means.labels_\n",
    "pca2 = PCA(n_components=3).fit(df_players)\n",
    "pca2d = pca2.transform(df_players)\n",
    "plt.figure(figsize = (10,10))\n",
    "sns.scatterplot(pca2d[:,0], pca2d[:,1], \n",
    "                hue=labels_scale, \n",
    "                palette='Set1',\n",
    "                s=100, alpha=0.2).set_title('KMeans Clusters (4) Derived from Original Dataset', fontsize=15)\n",
    "plt.legend()\n",
    "plt.ylabel('PC2')\n",
    "plt.xlabel('PC1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4. Plot a 3-D graph of the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scene = dict(xaxis = dict(title  = 'PC1'),yaxis = dict(title  = 'PC2'),zaxis = dict(title  = 'PC3'))\n",
    "labels = labels_scale\n",
    "trace = go.Scatter3d(x=pca2d[:,0], y=pca2d[:,1], z=pca2d[:,2], mode='markers',marker=dict(color = labels, colorscale='Viridis', size = 10, line = dict(color = 'gray',width = 5)))\n",
    "layout = go.Layout(margin=dict(l=0,r=0),scene = Scene, height = 1000,width = 1000)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Applying PCA to K-means "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_components=900 because we have 900 features in the dataset\n",
    "pca = PCA(n_components=900)\n",
    "pca.fit(df_players)\n",
    "variance = pca.explained_variance_ratio_\n",
    "var = np.cumsum(np.round(variance, 3)*100)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Features')\n",
    "plt.title('PCA Analysis')\n",
    "plt.ylim(0,100.5)\n",
    "plt.plot(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Examine the n components with a value of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca_scale = pca.fit_transform(df_players)\n",
    "pca_df_scale = pd.DataFrame(pca_scale,  columns=['pc1','pc2'])\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Evaluate the elbow method distribution using PCA (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "k_list = range(1, 15)\n",
    "\n",
    "for k in k_list:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(pca_df_scale)\n",
    "    sse.append([k, km.inertia_])\n",
    "    \n",
    "pca_results_scale = pd.DataFrame({'Cluster': range(1,15), 'SSE': sse})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1], marker='o', color='green')\n",
    "plt.title('Optimal Number of Clusters using Elbow Method (PCA_Scaled Data)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. After applying PCA (2), recalculate the silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca_scale = KMeans(n_clusters=4, n_init=100, max_iter=400, init='k-means++', random_state=42).fit(pca_df_scale)\n",
    "\n",
    "print('KMeans PCA Scaled Silhouette Score: {}'.format(silhouette_score(pca_df_scale, kmeans_pca_scale.labels_, metric='euclidean')))\n",
    "labels_pca_scale = kmeans_pca_scale.labels_\n",
    "clusters_pca_scale = pd.concat([pca_df_scale, \n",
    "                                pd.DataFrame({'pca_clusters':labels_pca_scale})], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Examine the n components with a value of 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_scale = pca.fit_transform(df_players)\n",
    "pca_df_scale = pd.DataFrame(pca_scale,  columns=['pc1','pc2','pc3'])\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. Evaluate the elbow method distribution using PCA (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "k_list = range(1, 15)\n",
    "\n",
    "for k in k_list:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(pca_df_scale)\n",
    "    sse.append([k, km.inertia_])\n",
    "    \n",
    "pca_results = pd.DataFrame({'Cluster': range(1,15), 'SSE': sse})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1], marker='o', color='green')\n",
    "plt.title('Optimal Number of Clusters using Elbow Method (PCA_Scaled Data)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6.  After applying PCA (3), recalculate the silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca_scale = KMeans(n_clusters=4, n_init=100, max_iter=400, init='k-means++', random_state=42).fit(pca_df_scale)\n",
    "\n",
    "print('KMeans PCA Scaled Silhouette Score: {}'.format(silhouette_score(pca_df_scale, kmeans_pca_scale.labels_, metric='euclidean')))\n",
    "labels_pca_scale = kmeans_pca_scale.labels_\n",
    "clusters_pca_scale = pd.concat([pca_df_scale, \n",
    "                                pd.DataFrame({'pca_clusters':labels_pca_scale})], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Examine the n components with a value of 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=4)\n",
    "pca_scale = pca.fit_transform(df_players)\n",
    "pca_df_scale = pd.DataFrame(pca_scale,  columns=['pc1','pc2','pc3','pc4'])\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. Evaluate the elbow method distribution using PCA (4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "k_list = range(1, 15)\n",
    "\n",
    "for k in k_list:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(pca_df_scale)\n",
    "    sse.append([k, km.inertia_])\n",
    "    \n",
    "pca_results = pd.DataFrame({'Cluster': range(1,15), 'SSE': sse})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1], marker='o', color='green')\n",
    "plt.title('Optimal Number of Clusters using Elbow Method (PCA_Scaled Data)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.9. After applying PCA (4), recalculate the silhouette score using K-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca_scale = KMeans(n_clusters=4, n_init=100, max_iter=400, init='k-means++', random_state=42).fit(pca_df_scale)\n",
    "\n",
    "print('KMeans PCA Scaled Silhouette Score: {}'.format(silhouette_score(pca_df_scale, kmeans_pca_scale.labels_, metric='euclidean')))\n",
    "labels_pca_scale = kmeans_pca_scale.labels_\n",
    "clusters_pca_scale = pd.concat([pca_df_scale, \n",
    "                                pd.DataFrame({'pca_clusters':labels_pca_scale})], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.10. Examine the n components with a value of 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=5)\n",
    "pca_scale = pca.fit_transform(df_players)\n",
    "pca_df_scale = pd.DataFrame(pca_scale,  columns=['pc1','pc2','pc3','pc4','pc5'])\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.11. Evaluate the elbow method distribution using PCA (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "k_list = range(1, 15)\n",
    "\n",
    "for k in k_list:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(pca_df_scale)\n",
    "    sse.append([k, km.inertia_])\n",
    "    \n",
    "pca_results = pd.DataFrame({'Cluster': range(1,15), 'SSE': sse})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1], marker='o', color='green')\n",
    "plt.title('Optimal Number of Clusters using Elbow Method (PCA_Scaled Data)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.12. After applying PCA (5), recalculate the silhouette score using K-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca_scale = KMeans(n_clusters=4, n_init=100, max_iter=400, init='k-means++', random_state=42).fit(pca_df_scale)\n",
    "\n",
    "print('KMeans PCA Scaled Silhouette Score: {}'.format(silhouette_score(pca_df_scale, kmeans_pca_scale.labels_, metric='euclidean')))\n",
    "labels_pca_scale = kmeans_pca_scale.labels_\n",
    "clusters_pca_scale = pd.concat([pca_df_scale, \n",
    "                                pd.DataFrame({'pca_clusters':labels_pca_scale})], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.13. Examine the n components with a value of 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=30)\n",
    "pca_scale = pca.fit_transform(df_players)\n",
    "pca_df_scale = pd.DataFrame(pca_scale)\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.14. Evaluate the elbow method distribution using PCA (30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "k_list = range(1, 15)\n",
    "\n",
    "for k in k_list:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(pca_df_scale)\n",
    "    sse.append([k, km.inertia_])\n",
    "    \n",
    "pca_results = pd.DataFrame({'Cluster': range(1,15), 'SSE': sse})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1], marker='o', color='green')\n",
    "plt.title('Optimal Number of Clusters using Elbow Method (PCA_Scaled Data)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.15. After applying PCA (30), recalculate the silhouette score using K-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca_scale = KMeans(n_clusters=4, n_init=100, max_iter=400, init='k-means++', random_state=42).fit(pca_df_scale)\n",
    "\n",
    "print('KMeans PCA Scaled Silhouette Score: {}'.format(silhouette_score(pca_df_scale, kmeans_pca_scale.labels_, metric='euclidean')))\n",
    "labels_pca_scale = kmeans_pca_scale.labels_\n",
    "clusters_pca_scale = pd.concat([pca_df_scale, \n",
    "                                pd.DataFrame({'pca_clusters':labels_pca_scale})], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA n_components and silhoutte score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=[{'Number of PCA':2,\n",
    "         'Number of Clusters': 4,\n",
    "         'Silhouette Score': 0.411\n",
    "         },\n",
    "        {\n",
    "         'Number of PCA':3,\n",
    "         'Number of Clusters': 4,\n",
    "         'Silhouette Score': 0.458\n",
    "        },\n",
    "        {'Number of PCA':4,\n",
    "         'Number of Clusters': 4,\n",
    "         'Silhouette Score': 0.410\n",
    "         },\n",
    "        {'Number of PCA':5,\n",
    "         'Number of Clusters': 4,\n",
    "         'Silhouette Score': 0.374\n",
    "         },\n",
    "        {'Number of PCA':30,\n",
    "         'Number of Clusters': 4,\n",
    "         'Silhouette Score': 0.158\n",
    "         },]\n",
    "df=pd.DataFrame(pca, index=['Princpal Component Analysis','Princpal Component Analysis','Princpal Component Analysis','Princpal Component Analysis', 'Princpal Component Analysis'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.16. For the K-means, PCA with a value of 3 produced the best silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_scale = pca.fit_transform(df_players)\n",
    "pca_df_scale = pd.DataFrame(pca_scale,  columns=['pc1','pc2','pc3'])\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.17. Evaluate the elbow method distribution using PCA (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "k_list = range(1, 15)\n",
    "\n",
    "for k in k_list:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(pca_df_scale)\n",
    "    sse.append([k, km.inertia_])\n",
    "    \n",
    "pca_results = pd.DataFrame({'Cluster': range(1,15), 'SSE': sse})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1], marker='o', color='green')\n",
    "plt.title('Optimal Number of Clusters using Elbow Method (PCA_Scaled Data)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.18. After applying PCA (3), recalculate the silhouette score using K-means++\n",
    "I tried a few other numbers for the n component, but it appears that **0.458** is the highest possible score for the silhouette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca_scale = KMeans(n_clusters=4, n_init=100, max_iter=400, init='k-means++', random_state=42).fit(pca_df_scale)\n",
    "\n",
    "print('KMeans PCA Scaled Silhouette Score: {}'.format(silhouette_score(pca_df_scale, kmeans_pca_scale.labels_, metric='euclidean')))\n",
    "labels_pca_scale = kmeans_pca_scale.labels_\n",
    "clusters_pca_scale = pd.concat([pca_df_scale, \n",
    "                                pd.DataFrame({'pca_clusters':labels_pca_scale})], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.19. Hyperparameter tunning on K-means after applying PCA (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_clusters': [2, 3, 4, 5, 10, 20, 30]}\n",
    "\n",
    "parameter_grid = ParameterGrid(parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(parameter_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = -1\n",
    "model = KMeans()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.20. Fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in parameter_grid:\n",
    "    model.set_params(**g)\n",
    "    model.fit(pca_df_scale)\n",
    "\n",
    "    ss = metrics.silhouette_score(pca_df_scale, model.labels_)\n",
    "    print('Parameter: ', g, 'Score: ', ss)\n",
    "    if ss > best_score:\n",
    "        best_score = ss\n",
    "        best_grid = g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.21. The best silhouette score for K-means++ algorithm is (4) clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.22. Present a graph that was derived from PCA (3) using K-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (10,10))\n",
    "sns.scatterplot(clusters_pca_scale.iloc[:,0],\n",
    "                clusters_pca_scale.iloc[:,1], \n",
    "                hue=labels_pca_scale, palette='Set1', s=100, \n",
    "                alpha=0.2).set_title('KMeans Clusters (4) Derived from PCA', fontsize=15)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.23. Plot a 3-D graph of K-means clusters derived from PCA (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scene = dict(xaxis = dict(title  = 'PC1'),yaxis = dict(title  = 'PC2'),zaxis = dict(title  = 'PC3'))\n",
    "labels = labels_scale\n",
    "trace = go.Scatter3d(x=pca2d[:,0], y=pca2d[:,1], z=pca2d[:,2], mode='markers',marker=dict(color = labels, colorscale='Viridis', size = 10, line = dict(color = 'gray',width = 5)))\n",
    "layout = go.Layout(margin=dict(l=0,r=0),scene = Scene, height = 1000,width = 1000)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DBSCAN Algorithm\n",
    "\n",
    "DBSCAN - Density-Based Spatial Clustering of Applications with Noise. Finds core samples of high density and expands clusters from them. Good for data which contains clusters of similar density.\n",
    "Dense region, Sparse region, Core point, Border point, Noise point , Density edge , Density connected points\n",
    "\n",
    "The DBSCAN algorithm uses two parameters:\n",
    "\n",
    "**min points:** The minimum number of points (a threshold) clustered together for a region to be considered dense.\n",
    "\n",
    "**epsilon (ε):** A distance measure that will be used to locate the points in the neighborhood of any point.\n",
    "\n",
    "The following is the analytical strategy used in the DBSCAN algorithm:\n",
    "\n",
    "\n",
    "### 1. Apply elbow method using nearest neighbors\n",
    "### 2. Applying DBSCAN to the original dataset\n",
    "### 3. Construct a 3-D graph to illustrate each cluster distribution.\n",
    "### 4. Applying PCA to DBSCAN (original dataset)\n",
    "### 5. Hyperparameter tuning for DBSCAN(epsilon & minimum points)\n",
    "### 6. Save the results to a file and then analyse them after adjusting the PCA parameters.\n",
    "### 7. Construct a 3-D graph to illustrate each cluster distribution.\n",
    "### 8. Apply a value range of (2,3, & 4) for the PCA n component and examine the results\n",
    "### 9. Eliminate the rows containing noise (-1), store the model label back into a dataframe object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca.fit(df_players)\n",
    "pca_scale = pca.transform(df_players)\n",
    "pca_df = pd.DataFrame(pca_scale, columns=['pc1', 'pc2', 'pc3'])\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Applying the elbow method using nearest neighbors (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use nearestneighbors for calculating distance between points\n",
    "neigh=NearestNeighbors(n_neighbors=2)\n",
    "distance=neigh.fit(pca_df)\n",
    "distances,indices=distance.kneighbors(pca_df)\n",
    "sorting_distances=np.sort(distances,axis=0)\n",
    "sorted_distances=sorting_distances[:,1]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(sorted_distances)\n",
    "plt.xlabel('Distance')\n",
    "plt.ylabel('Epsilon')\n",
    "plt.axhline(y=0.2, color='red', ls='--')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Applying DBSCAN to the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=0.2, min_samples=4).fit(pca_df)\n",
    "labels = dbscan.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(pca_df, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Construct a 3-D graph to illustrate each cluster distribution (original dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scene = dict(xaxis = dict(title  = 'PC1'),yaxis = dict(title  = 'PC2'),zaxis = dict(title  = 'PC3'))\n",
    "trace = go.Scatter3d(x=pca_df.iloc[:,0], y=pca_df.iloc[:,1], z=pca_df.iloc[:,2],\n",
    "                     mode='markers',marker=dict(colorscale='Greys', opacity=0.3, size = 10, ))\n",
    "layout = go.Layout(margin=dict(l=0,r=0),scene = Scene, height = 1000,width = 1000)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.update_layout(title='DBSCAN clusters Derived from Original Data', font=dict(size=12,))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Applying PCA (3) to DBSCAN (original dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dbscan = PCA(n_components=3)\n",
    "pca_dbscan.fit(df_players)\n",
    "pca_scale_dbscan = pca_dbscan.transform(df_players)\n",
    "pca_df = pd.DataFrame(pca_scale_dbscan, columns=['pc1', 'pc2', 'pc3'])\n",
    "print(pca_dbscan.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Hyperparameter tuning for DBSCAN (epsilon & minimum points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_eps_values = np.arange(0.2,2.6,0.1) \n",
    "pca_min_samples = np.arange(2,11) \n",
    "pca_dbscan_params = list(product(pca_eps_values, pca_min_samples))\n",
    "pca_no_of_clusters = []\n",
    "pca_sil_score = []\n",
    "pca_epsvalues = []\n",
    "pca_min_samp = []\n",
    "for p in pca_dbscan_params:\n",
    "    pca_dbscan_cluster = DBSCAN(eps=p[0], min_samples=p[1]).fit(pca_df)\n",
    "    pca_epsvalues.append(p[0])\n",
    "    pca_min_samp.append(p[1])\n",
    "    pca_no_of_clusters.append(len(np.unique(pca_dbscan_cluster.labels_)))\n",
    "    pca_sil_score.append(silhouette_score(pca_df, pca_dbscan_cluster.labels_))\n",
    "pca_eps_min = list(zip(pca_no_of_clusters, pca_sil_score, pca_epsvalues, pca_min_samp))\n",
    "pca_eps_min_df = pd.DataFrame(pca_eps_min, columns=['no_of_clusters', 'silhouette_score', 'epsilon_values', 'minimum_points'])\n",
    "pca_eps_min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Save the result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca_eps_min_df).to_csv('dbscanresultpca1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 I evaluated the obtained result to fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.3, min_samples=4).fit(pca_df)\n",
    "labels = dbscan.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(pca_df, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Construct a 3-D graph to illustrate (4) cluster distributions using the above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scene = dict(xaxis = dict(title  = 'PC1'),yaxis = dict(title  = 'PC2'),zaxis = dict(title  = 'PC3'))\n",
    "labels = dbscan.labels_\n",
    "trace = go.Scatter3d(x=pca_df.iloc[:,0], y=pca_df.iloc[:,1], z=pca_df.iloc[:,2], mode='markers', marker=dict(color = labels, colorscale='Viridis', size = 10, line = dict(color = 'gray',width = 5)))\n",
    "layout = go.Layout(scene = Scene, height = 1000,width = 1000)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.update_layout(title='DBSCAN clusters (4) Derived from PCA', font=dict(size=12,))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Apply a value range of (2,3, & 4) for the PCA n component and examine the results\n",
    "Applying PCA (2) to DBSCAN (original dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dbscan = PCA(n_components=2)\n",
    "pca_dbscan.fit(df_players)\n",
    "pca_scale_dbscan = pca_dbscan.transform(df_players)\n",
    "pca_df = pd.DataFrame(pca_scale_dbscan, columns=['pc1', 'pc2'])\n",
    "print(pca_dbscan.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1. Calculate the epsilon and minimum point parameters while simultaneously eliminating noise to produce the silhouette coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=2, min_samples=2).fit(pca_df)\n",
    "labels = dbscan.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(pca_df, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.2. Hyperparameter tuning for DBSCAN (epsilon & minimum points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_eps_values = np.arange(0.2,2.1,0.1) \n",
    "pca_min_samples = np.arange(2,11) \n",
    "pca_dbscan_params = list(product(pca_eps_values, pca_min_samples))\n",
    "pca_no_of_clusters = []\n",
    "pca_sil_score = []\n",
    "pca_epsvalues = []\n",
    "pca_min_samp = []\n",
    "for p in pca_dbscan_params:\n",
    "    pca_dbscan_cluster = DBSCAN(eps=p[0], min_samples=p[1]).fit(pca_df)\n",
    "    pca_epsvalues.append(p[0])\n",
    "    pca_min_samp.append(p[1])\n",
    "    pca_no_of_clusters.append(len(np.unique(pca_dbscan_cluster.labels_)))\n",
    "    pca_sil_score.append(silhouette_score(pca_df, pca_dbscan_cluster.labels_))\n",
    "pca_eps_min = list(zip(pca_no_of_clusters, pca_sil_score, pca_epsvalues, pca_min_samp))\n",
    "pca_eps_min_df = pd.DataFrame(pca_eps_min, columns=['no_of_clusters', 'silhouette_score', 'epsilon_values', 'minimum_points'])\n",
    "pca_eps_min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.3. Save the result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca_eps_min_df).to_csv('dbscanresultpca2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.4. I evaluated the obtained result to fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.2, min_samples=2).fit(pca_df)\n",
    "labels = dbscan.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(pca_df, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.5. Construct a 3-D graph to illustrate (4) cluster distributions using the above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scene = dict(xaxis = dict(title  = 'PC1'),yaxis = dict(title  = 'PC2'),zaxis = dict(title  = 'PC3'))\n",
    "labels = dbscan.labels_\n",
    "trace = go.Scatter3d(x=pca_df.iloc[:,0], y=pca_df.iloc[:,1], z=pca_df.iloc[:,2], mode='markers', marker=dict(color = labels, colorscale='Viridis', size = 10, line = dict(color = 'gray',width = 5)))\n",
    "layout = go.Layout(scene = Scene, height = 1000,width = 1000)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.update_layout(title='DBSCAN clusters (4) Derived from PCA', font=dict(size=12,))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.6. Applying PCA (4) to DBSCAN (original dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dbscan = PCA(n_components=4)\n",
    "pca_dbscan.fit(df_players)\n",
    "pca_scale_dbscan = pca_dbscan.transform(df_players)\n",
    "pca_df = pd.DataFrame(pca_scale_dbscan, columns=['pc1', 'pc2','pc3','pc4'])\n",
    "print(pca_dbscan.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.7. Calculate the epsilon and minimum point parameters while simultaneously eliminating noise to produce the silhouette coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=3.3, min_samples=2).fit(pca_df)\n",
    "labels = dbscan.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(pca_df, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.8 Hyperparameter tuning for DBSCAN (epsilon & minimum points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_eps_values = np.arange(0.2,3.3,0.1) \n",
    "pca_min_samples = np.arange(2,11) \n",
    "pca_dbscan_params = list(product(pca_eps_values, pca_min_samples))\n",
    "pca_no_of_clusters = []\n",
    "pca_sil_score = []\n",
    "pca_epsvalues = []\n",
    "pca_min_samp = []\n",
    "for p in pca_dbscan_params:\n",
    "    pca_dbscan_cluster = DBSCAN(eps=p[0], min_samples=p[1]).fit(pca_df)\n",
    "    pca_epsvalues.append(p[0])\n",
    "    pca_min_samp.append(p[1])\n",
    "    pca_no_of_clusters.append(len(np.unique(pca_dbscan_cluster.labels_)))\n",
    "    pca_sil_score.append(silhouette_score(pca_df, pca_dbscan_cluster.labels_))\n",
    "pca_eps_min = list(zip(pca_no_of_clusters, pca_sil_score, pca_epsvalues, pca_min_samp))\n",
    "pca_eps_min_df = pd.DataFrame(pca_eps_min, columns=['no_of_clusters', 'silhouette_score', 'epsilon_values', 'minimum_points'])\n",
    "pca_eps_min_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.9. Save the result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca_eps_min_df).to_csv('dbscanresultpca3.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.10. I evaluated the obtained result to fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.7, min_samples=3).fit(pca_df)\n",
    "labels = dbscan.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(pca_df, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.11. Construct a 3-D graph to illustrate (2) cluster distributions using the above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scene = dict(xaxis = dict(title  = 'PC1'),yaxis = dict(title  = 'PC2'),zaxis = dict(title  = 'PC3'))\n",
    "labels = dbscan.labels_\n",
    "trace = go.Scatter3d(x=pca_df.iloc[:,0], y=pca_df.iloc[:,1], z=pca_df.iloc[:,2], mode='markers',marker=dict(color = labels, colorscale='Viridis', size = 10, line = dict(color = 'gray',width = 5)))\n",
    "layout = go.Layout(scene = Scene, height = 1000,width = 1000)\n",
    "data = [trace]\n",
    "fig = go.Figure(data = data, layout = layout)\n",
    "fig.update_layout(title=\"'DBSCAN Clusters (4) Derived from PCA'\", font=dict(size=12,))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.12. For the DBSCAN, PCA with n component value of 2 produced the best silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_dbscan = PCA(n_components=2)\n",
    "pca_dbscan.fit(df_players)\n",
    "pca_scale_dbscan = pca_dbscan.transform(df_players)\n",
    "pca_df = pd.DataFrame(pca_scale_dbscan)\n",
    "print(pca_dbscan.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.13. Evaluate the obtained result to fine-tune the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbscan = DBSCAN(eps=1.2, min_samples=2).fit(pca_df)\n",
    "labels = dbscan.labels_\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "print('Estimated number of noise points: %d' % n_noise_)\n",
    "print(\"Silhouette Coefficient: %0.3f\" % metrics.silhouette_score(pca_df, labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Eliminate the rows containing noise (-1), store the model label back into a dataframe object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players[\"Labels\"] = labels\n",
    "df_players.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1.  Eliminate rows with noise using DataFrame Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_noise_ = list(labels).count(-1)\n",
    "print('Count:', n_noise_)\n",
    "indexNames = df_players[df_players['Labels'] == -1 ].index\n",
    "df_players.drop(indexNames , inplace=True)\n",
    "df_players.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.2. Confirming the size of the dataset after dropping 5 rows with noise (-1)\n",
    "**After removing the noisy datapoint, the dataset has been altered. I'll have to re-import the dataset in order to build a classification model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.3. Examine the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players['Labels'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Model\n",
    "\n",
    "This is the second phase of the experiment that involves building a machine learning pipeline using _**Support Vector Machine**_ and _**Random Forest**_. I am going to use the K-means algorithm because the results were fascinating from a business point of view. All of the clusters in the K-means were evenly distributed, providing a strong understanding of the football players. \n",
    "\n",
    "DBSCAN results were not even dispersed; over 90% of the football players were concentrated in a single cluster. This would not contribute to the model's core functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the size of the dataset to ensure that no changes occurred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for NaN (Not a Number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To remove columns that will not be used in the model, I need replace the column identifier ignoring spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.columns = [c.replace(' ', '') for c in data.columns]\n",
    "data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop any column that aren't necessary for the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop(['Name','Unnamed:0','ID','Photo','Flag','Overall','ClubLogo', 'Special', 'InternationalReputation', 'WeakFoot',\n",
    "               'SkillMoves','WorkRate','BodyType','RealFace','JerseyNumber','Joined','LoanedFrom','ContractValidUntil',\n",
    "                'Weight','Crossing','Finishing','HeadingAccuracy','ShortPassing','Volleys','Dribbling','Curve','FKAccuracy',\n",
    "                'LongPassing','BallControl','Acceleration','SprintSpeed','Agility','Reactions','Balance','ShotPower','Jumping',\n",
    "                'Stamina','Strength','LongShots','Aggression','Interceptions','Positioning','Vision','Penalties','Composure',\n",
    "                'Marking','SlidingTackle','StandingTackle','ReleaseClause'], axis=1)\n",
    "\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill up the empty rows (NaN) with the dataset's mean value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_means = data.mean()\n",
    "data = data.fillna(column_means)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove the Pounds symbol and letters from the players' wages and values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Wage = data.Wage.str.replace(\"€\",\"\")\n",
    "data.Wage = data.Wage.str.replace(\"K\",\"\").astype(\"float\")\n",
    "data.Wage.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.Value = data.Value.str.replace(\"€\",\"\")\n",
    "data.Value = data.Value.str.replace(\"M\",\"\")\n",
    "data.Value = data.Value.str.replace(\"K\",\"\").astype(\"float\")\n",
    "data.Value.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encoding\n",
    "This is used to convert all categorical variable into indicator variable i.e., (0's and 1's)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummies=pd.get_dummies(data)\n",
    "dummies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Store the dummy method into variable X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=dummies\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "players_scale = preprocessing.StandardScaler().fit(X).transform(X)\n",
    "players_scale[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players = pd.DataFrame(players_scale, columns=X.columns)\n",
    "df_players.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA to K-means++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#n_components=900 because we have 900 features in the dataset\n",
    "pca = PCA(n_components=900)\n",
    "pca.fit(df_players)\n",
    "variance = pca.explained_variance_ratio_\n",
    "var = np.cumsum(np.round(variance, 3)*100)\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.ylabel('% Variance Explained')\n",
    "plt.xlabel('# of Features')\n",
    "plt.title('PCA Analysis')\n",
    "plt.ylim(0,100.5)\n",
    "plt.plot(var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the n components with a value of 3 given that it produced the best silhouette score previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "pca_scale = pca.fit_transform(df_players)\n",
    "pca_df_scale = pd.DataFrame(pca_scale,  columns=['pc1','pc2','pc3'])\n",
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying silhouette coefficient (using the elbow method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sse = []\n",
    "k_list = range(1, 15)\n",
    "\n",
    "for k in k_list:\n",
    "    km = KMeans(n_clusters=k)\n",
    "    km.fit(pca_df_scale)\n",
    "    sse.append([k, km.inertia_])\n",
    "    \n",
    "pca_results = pd.DataFrame({'Cluster': range(1,15), 'SSE': sse})\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(pd.DataFrame(sse)[0], pd.DataFrame(sse)[1], marker='o', color='green')\n",
    "plt.title('Optimal Number of Clusters using Elbow Method (PCA_Scaled Data)')\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Inertia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applying PCA to K-means++\n",
    "Recall that PCA component of 3 gave us the best silhouette score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans_pca_scale = KMeans(n_clusters=4, n_init=100, max_iter=400, init='k-means++', random_state=42).fit(pca_df_scale)\n",
    "\n",
    "print('KMeans PCA Scaled Silhouette Score: {}'.format(silhouette_score(pca_df_scale, kmeans_pca_scale.labels_, metric='euclidean')))\n",
    "labels_pca_scale = kmeans_pca_scale.labels_\n",
    "clusters_pca_scale = pd.concat([pca_df_scale, \n",
    "                                pd.DataFrame({'pca_clusters':labels_pca_scale})], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the K-means++ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterNum = 4\n",
    "k_means = KMeans(init = \"k-means++\", n_clusters = clusterNum, n_init = 12)\n",
    "k_means.fit(pca_df_scale)\n",
    "labels = k_means.labels_\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a new column for the clustered labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players[\"Clusters\"] = labels\n",
    "df_players.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine the size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_players.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separate the data into features (X) and targets (clusters (y))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_players.drop('Clusters', axis=1)\n",
    "y = df_players['Clusters']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Over-sampling and under-sampling on unbalanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(imblearn.__version__)\n",
    "\n",
    "oversample = SMOTE()\n",
    "enn = EditedNearestNeighbours()\n",
    "# label encode the target variable\n",
    "\n",
    "y = LabelEncoder().fit_transform(y)\n",
    "\n",
    "X, y = enn.fit_resample(X, y)\n",
    "# summarize distribution\n",
    "counter = Counter(y)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(y) * 100\n",
    "\n",
    "    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\n",
    "#plot the distribution\n",
    "plt.bar(counter.keys(), counter.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split divides the dataset into 70 percent for training and 30 percent for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split( X, y, test_size=0.3, random_state=50)\n",
    "print ('Train set:', x_train.shape,  y_train.shape)\n",
    "print ('Test set:', x_test.shape,  y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights on the clustering pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.countplot(x = df_players['Clusters'])\n",
    "plt.figure(figsize=(80, 40))\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=40, ha=\"right\")\n",
    "ax.set_xlabel('Clusters') \n",
    "ax.set_ylabel('Number of players')\n",
    "plt.tight_layout()\n",
    "#plt.title(\"Visualization of players based on their position\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine \n",
    "To build this model, I have used the Support Vector Machine Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search cross validation hyperparameter tuning will be used to improve our model's performance accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining parameter range\n",
    "param_grid = {'C': [0.1, 1, 10, 100, 1000], \n",
    "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
    "              'kernel': ['rbf']} \n",
    "  \n",
    "grid = GridSearchCV(clf, param_grid, refit = True, verbose = 3)\n",
    "  \n",
    "# fitting the model for grid search\n",
    "grid.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will produce the best parameters, estimator, and score for the SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameter:',grid.best_params_)\n",
    "print('Grid best estimator:',grid.best_estimator_)\n",
    "print('Best score:',grid.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will now, apply the above-mentioned parameters for the SVM classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(C=1, gamma=0.0001, kernel= 'rbf')\n",
    "clf.fit(x_train, y_train) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Will apply the predict method to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I used cross validation to further analyze the model's performance on its test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('Accuracy of SVM classifier on the training set: {:.2f}'.format(clf.score(x_train, y_train)))\n",
    "print('Accuracy of SVM classifier on the test set: {:.2f}'.format(clf.score(x_test, y_test)))\n",
    "\n",
    "#Decision Trees are very prone to overfitting as shown in the scores\n",
    "\n",
    "score = cross_val_score(clf, x_train, y_train, cv=10) \n",
    "print('Cross-validation score: ',score)\n",
    "print('Cross-validation mean score: ',score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the model's performance using different classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_classification(y_test,y_pred,avg_method='weighted'):\n",
    "    acc = accuracy_score(y_test, y_pred,normalize=True)\n",
    "    num_acc = accuracy_score(y_test, y_pred,normalize=False)\n",
    "    f1= f1_score(y_test, y_pred, average=avg_method)\n",
    "    prec = precision_score(y_test, y_pred, average=avg_method)\n",
    "    recall = recall_score(y_test, y_pred, average=avg_method)\n",
    "    jaccard = jaccard_score(y_test, y_pred, average=avg_method)\n",
    "    \n",
    "    print(\"Length of testing data: \", len(y_test))\n",
    "    print(\"accuracy_count : \" , num_acc)\n",
    "    print(\"accuracy_score : \" , acc)\n",
    "    print(\"f1_score : \" , f1)\n",
    "    print(\"precision_score : \" , prec)\n",
    "    print(\"recall_score : \", recall)\n",
    "    print(\"jaccard_score : \", jaccard)\n",
    "    \n",
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To further evaluate our performance findings, let's build a confusion matrix that describes the false positive, true negative, true positive, and false negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print (classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['False(0)','True(1)'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Summary of the confusion matrix\n",
    "Looking at the first row. The first row contains players whose false value in the test set is (0). As you can see, 844 of the 1,443 players have a false value of (0). And, of these 844, the classifier accurately predicted 843 as (0), and 1 as (1) (True value) for the predicted labels.\n",
    "\n",
    "This indicates that in the test set, the actual false value for 843 players was (0), and the classifier accurately predicted those as (0) (True label). However, while the actual label of 1 players was 1 (False value), the classifier predicted those as 1, which means it did excellently well. We may think of it as a model excellence for the first row.\n",
    "\n",
    "What about the players that have a true value of 1?  \n",
    "\n",
    "Looking at the second row. It appears that there are 599 players whose true value was 1. The classifier accurately identified 599 of them as 1 as a result, but incorrectly predicted only 1 of them as 0 (False value). it has done an excellent job at predicting players with true value 1. The confusion matrix is useful since it demonstrates the model's ability to properly predict or separate the classes. In the case of a binary classifier, such as this one, these values can be interpreted as the number of true positives, false positives, true negatives, and false negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the actual test set to the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "pred_results = pd.DataFrame({'y_test': pd.Series(y_test),\n",
    "                             'y_pred': pd.Series(y_pred)})\n",
    "\n",
    "pred_results.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier()\n",
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search cross validation hyperparameter tuning will be used to improve our model's performance accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter grid based on the results of random search \n",
    "param_grid = {\n",
    "    'bootstrap': [True],\n",
    "    'max_depth': [80, 90, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [100, 200, 300, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "                          cv = 10, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This will reccommend the best parameters, estimator, and score for the Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best parameter:', grid_search.best_params_)\n",
    "print('Best grid estimator:', grid_search.best_estimator_)\n",
    "print('Best score', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I applied the above-mentioned parameters for the Random Forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf=RandomForestClassifier(bootstrap=True, max_depth=100, max_features=3, \n",
    "                          min_samples_leaf=3, min_samples_split=8,\n",
    "                          n_estimators=100).fit(x_train,y_train)\n",
    "rf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I applied the predict method to the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = rf.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I used cross validation to further analyze the model's performance on its test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print('Accuracy of Random Forest classifier on the training set: {:.2f}'.format(rf.score(x_train, y_train)))\n",
    "print('Accuracy of Random Forest classifier on the test set: {:.2f}'.format(rf.score(x_test, y_test)))\n",
    "\n",
    "#Decision Trees are very prone to overfitting as shown in the scores\n",
    "\n",
    "score = cross_val_score(rf, x_train, y_train, cv=5) \n",
    "print('Cross-validation score: ',score)\n",
    "print('Cross-validation mean score: ',score.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize the model's performance using different classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_classification(y_test,y_pred,avg_method='weighted'):\n",
    "    acc = accuracy_score(y_test, y_pred,normalize=True)\n",
    "    num_acc = accuracy_score(y_test, y_pred,normalize=False)\n",
    "    f1= f1_score(y_test, y_pred, average=avg_method)\n",
    "    prec = precision_score(y_test, y_pred, average=avg_method)\n",
    "    recall = recall_score(y_test, y_pred, average=avg_method)\n",
    "    jaccard = jaccard_score(y_test, y_pred, average=avg_method)\n",
    "    \n",
    "    print(\"Length of testing data: \", len(y_test))\n",
    "    print(\"accuracy_count : \" , num_acc)\n",
    "    print(\"accuracy_score : \" , acc)\n",
    "    print(\"f1_score : \" , f1)\n",
    "    print(\"precision_score : \" , prec)\n",
    "    print(\"recall_score : \", recall)\n",
    "    print(\"jaccard_score : \", jaccard)\n",
    "    \n",
    "summarize_classification(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of the actual test set to the predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy\n",
    "pred_results = pd.DataFrame({'y_test': pd.Series(y_test),\n",
    "                             'y_pred': pd.Series(y_pred)})\n",
    "\n",
    "pred_results.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To further evaluate our performance findings, let's build a confusion matrix that describes the false positive, true negative, true positive, and false negative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_test, y_pred, labels=[0,1])\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "print (classification_report(y_test, y_pred))\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=['False(0)','True(1)'],normalize= False,  title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary of the confusion matrix\n",
    "Looking at the first row. The first row contains players whose false value in the test set is (0). As you can see, 843 of the 1,441 players have a false value of (0). And, of these 843, the classifier accurately predicted 843 as (0), and 0 as (1) for the predicted label.\n",
    "\n",
    "This indicates that in the test set, the actual false value for 843 players was (0), and the classifier accurately predicted those as (0). However, while the actual label of 0 players was 0 (false value), the classifier predicted those as 1, which means it did excellently well. We may think of it as a model excellence for the first row.\n",
    "\n",
    "What about the players that have a true value of 1?  \n",
    "\n",
    "\n",
    "Looking at the second row. It appears that there are 599 players whose true value was 1. The classifier accurately identified 577 of them as 1, and 21 of them wrongly as 0 (false). As a result, it has done a good job at predicting players with true value 1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final review of the results and evaluations\n",
    "This provides an understanding of the business perspective related to our model. The clustering method (K-means) was able to categorise players according to their attributes. We may also conclude that the algorithm accurately identified the average, undervalued, and overperforming players. It also produced astounding results for the re-evaluation of the players, with 99 percent accuracy on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Clusters\"] = labels\n",
    "data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model should be saved to file for adequate evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(data).to_csv('clustereddata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=[{ 'Accuracy Score':'99%',\n",
    "         'F1 Score': '99%',\n",
    "         'Precision Score': '99%',\n",
    "         'Recall Score': '99%',\n",
    "         'Jaccard Score': '99%'},\n",
    "        {'Accuracy Score':'98%',\n",
    "         'F1 Score': '98%',\n",
    "         'Precision Score': '98%',\n",
    "         'Recall Score': '98%',\n",
    "         'Jaccard Score': '97%'}]\n",
    "df=pd.DataFrame(result, index=['Support Vector Machine','Random Forest'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means++ was the method adopted in this research.   At first, one could have assumed that the poor performance was due to the dataset's susceptibility to noise, large dimensionality, or even the cluster shape. The use of PCA on K-means++ has resulted in a more equitable and business-friendly solution. The K-means++ algorithm was able to satisfy the requirements of this research by providing managers with insight on player's skill diversity problems such as underperforming, undervalue, average, overperforming   among many others. The K-means++ method was successful in identifying possible groupings of players based of various attributes. Managers can now understand how the model works and make sound recommendations based on their preferences. The final phase of the project involves re-evaluating the players using a supervised machine learning technique. But, before I get into the next phase, let's have a look at the intelligent distribution of clusters that K-means++ has produced.\n",
    "\n",
    "Having said that, I went ahead and used DBSCAN, a density clustering technique commonly employed on non-linear or non-spherical datasets. Two parameters are required: epsilon and minimum points. I also used PCA to reduce the number of dimensions to 3 principal components. I estimated an epsilon value of 0.2 and a minimum point value of 4 using the elbow method. I was able to attain 72 clusters, 1406 noise, and a silhouette score of -0.55 by using this parameter. Admittedly, the findings were unimpressive. To fine-tune the epsilon and minimum points values, I have used an iterative approach. I chose an epsilon value of 1.2 and a minimum points value of 2. The method produced 6 valid clusters, 5 noises, and a silhouette score of 0.46. However, when the generated clusters were plotted, it was observed that the first cluster contained 90% of the players. Similarly, from a business perspective, I would like that the clusters be more evenly distributed in order to give us with useful information about the players. Perhaps DBSCAN is not the best clustering technique for this dataset.\n",
    "\n",
    "On the test set, the classification model generated excellent results. Because I have adequate data to train on, the model is not prone to overfitting. On the test set, Support Vector Machine and Random Forest achieve 98 and 99 percent accuracy, respectively. The f1 score and recall for all classes yielded 100 percent score for SVM classifier. This will address the manager's re-evaluation problem by predicting which category a given player’s skill set should belong. Managers can now determine if a player's release clause is genuinely worth the amount asked on the transfer market and which players should be rotated into other positions. This model has helped managers in diagnosing a lack of skill diversity and potentially influencing transfer decisions. Models now offers recommendation on players based on the manager's preferences."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
